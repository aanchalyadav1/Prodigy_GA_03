# -*- coding: utf-8 -*-
"""Text Generation with GPT-2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NIRKoTSUxErbZaPqA-MmW2pcLdNHiKNQ

# **1. Prepare the Dataset**

**Step 1: Download and Load the Raw Text**
"""

pip install transformers datasets torch tqdm

# Load the raw text
with open("text.txt", "r", encoding="utf-8") as file:
    raw_data = file.read()

# Display the first 500 characters
print(raw_data[:500])

"""**Step 2: Clean the Text**"""

import re

def clean_text(text):
    # Remove Project Gutenberg headers and footers
    text = re.sub(r"(\*\*\* START OF.*?\*\*\*)|(\*\*\* END OF.*?\*\*\*)", "", text, flags=re.DOTALL)
    # Remove special characters
    text = re.sub(r"[^\w\s.,;:!?'-]", "", text)
    # Normalize whitespace
    text = re.sub(r"\s+", " ", text).strip()
    return text

# Clean the raw text
cleaned_data = clean_text(raw_data)

# Save cleaned text
with open("cleaned_text.txt", "w", encoding="utf-8") as file:
    file.write(cleaned_data)

# Display the first 500 characters of the cleaned text
print(cleaned_data[:500])

"""**Step 3: Segment the Text**"""

# Segment text into sentences
segments = cleaned_data.split('.')
segments = [segment.strip() + "." for segment in segments if segment.strip()]

# Save segmented text
with open("segmented_text.txt", "w", encoding="utf-8") as file:
    file.writelines(segment + "\n" for segment in segments)

# Display first 5 segments
print(segments[:5])

"""#**2. Fine-Tune GPT-2**

****Step 1: Load Pre-trained GPT-2****
"""

from transformers import GPT2Tokenizer, GPT2LMHeadModel

# Load tokenizer and model
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
model = GPT2LMHeadModel.from_pretrained("gpt2")

# Add a special token for end-of-segment if needed
tokenizer.add_special_tokens({"pad_token": "<|pad|>"})
model.resize_token_embeddings(len(tokenizer))

"""****Step 2: Tokenize the Dataset****"""

from datasets import Dataset

# Create a dataset object
data = Dataset.from_dict({"text": segments})

# Tokenize the dataset
def tokenize_function(examples):
    return tokenizer(examples["text"], truncation=True, padding="max_length", max_length=128)

tokenized_data = data.map(tokenize_function, batched=True)

# Display an example of tokenized data
print(tokenized_data[0])

!pip install --upgrade transformers

"""**Step 3: Set Up the Training Arguments**"""

from transformers import TrainingArguments

# Optimized training arguments
training_args = TrainingArguments(
    output_dir="./gpt2-finetuned",
    overwrite_output_dir=True,
    eval_strategy="epoch",  # Replace with `eval_strategy`
    learning_rate=3e-5,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=2,
    num_train_epochs=1,
    save_steps=500,
    save_total_limit=1,
    logging_dir="./logs",
    logging_steps=50,
    fp16=True,
    push_to_hub=False,
)

"""**Step 4: Train the Model**"""

from transformers import Trainer, DataCollatorForLanguageModeling
from sklearn.model_selection import train_test_split

# Data collator for dynamic padding
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

# Split dataset into train and evaluation datasets (90% train, 10% eval)
train_texts, eval_texts = train_test_split(segments, test_size=0.1, random_state=42)

# Tokenize both datasets
train_data = Dataset.from_dict({"text": train_texts}).map(tokenize_function, batched=True)
eval_data = Dataset.from_dict({"text": eval_texts}).map(tokenize_function, batched=True)

# Trainer setup
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_data,
    eval_dataset=eval_data,  # Provide the evaluation dataset here
    data_collator=data_collator,
)

# Train the model
trainer.train()

"""# **3. Save and Test the Model**

**Step 1: Save the Fine-Tuned Model**
"""

# Save the fine-tuned model and tokenizer
model.save_pretrained("./gpt2-finetuned")
tokenizer.save_pretrained("./gpt2-finetuned")

"""**Step 2: Generate Text**"""

from transformers import pipeline

# Load the fine-tuned model
generator = pipeline("text-generation", model="./gpt2-finetuned", tokenizer=tokenizer)

# Generate text
prompt = "Dans la lumi√®re du matin,"
generated_text = generator(prompt, max_length=50, num_return_sequences=1)

# Display the generated text
print(generated_text[0]["generated_text"])
